"""
Inference script for automatic speech recognition models
Generated from template for {{ model_info.name }}
"""

import os
import json
import torch
import librosa
import numpy as np
from typing import Dict, Any

def load_model():
    """Load ASR model and processor"""
    from transformers import AutoProcessor, {{ model_type.model_class }}
    
    processor = AutoProcessor.from_pretrained("./model")
    model = {{ model_type.model_class }}.from_pretrained(
        "./model",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    return model, processor

def load_audio(audio_path: str) -> tuple:
    """Load and preprocess audio file
    
    Args:
        audio_path: Path to audio file
        
    Returns:
        Tuple of (waveform, sample_rate)
    """
    try:
        # Load audio file with target sample rate
        waveform, sample_rate = librosa.load(
            audio_path,
            sr={{ model_config.sample_rate or 16000 }}
        )
        
        # Convert to mono if stereo
        if len(waveform.shape) > 1:
            waveform = librosa.to_mono(waveform)
            
        return waveform, sample_rate
        
    except Exception as e:
        raise RuntimeError(f"Error loading audio file: {str(e)}")

def transcribe_audio(
    audio_path: str,
    model: Any,
    processor: Any
) -> Dict[str, Any]:
    """Transcribe audio file
    
    Args:
        audio_path: Path to audio file
        model: ASR model
        processor: ASR processor
        
    Returns:
        Dict containing transcription and metadata
    """
    # Load audio
    waveform, sample_rate = load_audio(audio_path)
    
    # Get parameters from environment or use defaults
    chunk_length_s = int(os.getenv("CHUNK_LENGTH", {{ model_config.chunk_length_s or 30 }}))
    return_timestamps = os.getenv("RETURN_TIMESTAMPS", "").lower() == "true"
    language = os.getenv("LANGUAGE", {{ model_config.language or "None" }})
    
    # Prepare processor inputs
    inputs = processor(
        waveform,
        sampling_rate=sample_rate,
        return_tensors="pt",
        chunk_length_s=chunk_length_s
    ).to(model.device)
    
    # Generate transcription
    with torch.no_grad():
        if language:
            outputs = model.generate(
                **inputs,
                language=language,
                task="transcribe",
                return_timestamps=return_timestamps
            )
        else:
            outputs = model.generate(
                **inputs, 
                task="transcribe",
                return_timestamps=return_timestamps
            )
    
    # Decode output tokens
    transcription = processor.batch_decode(
        outputs,
        skip_special_tokens=True
    )
    
    # Format results
    results = {
        "input_audio": audio_path,
        "transcription": transcription[0],
        "parameters": {
            "chunk_length_s": chunk_length_s,
            "return_timestamps": return_timestamps,
            "language": language,
            "sample_rate": sample_rate
        },
        "audio_metadata": {
            "duration": len(waveform) / sample_rate,
            "original_sample_rate": sample_rate
        }
    }
    
    # Add word-level timestamps if requested
    if return_timestamps and hasattr(outputs, "word_timestamps"):
        results["word_timestamps"] = [
            {
                "word": word,
                "start": float(start),
                "end": float(end)
            }
            for word, start, end in zip(
                outputs.word_timestamps.words,
                outputs.word_timestamps.starts,
                outputs.word_timestamps.ends
            )
        ]
    
    return results

def main():
    """Main inference function"""
    # Get input audio path
    audio_path = os.getenv("MODEL_INPUT", "/inputs/audio.wav")
    
    # Load model
    model, processor = load_model()
    
    # Run transcription
    results = transcribe_audio(audio_path, model, processor)
    
    # Save results
    output_file = os.path.join("/outputs", "results.json")
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    main()