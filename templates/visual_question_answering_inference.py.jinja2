"""
Inference script for visual question answering models
Generated for {{ model_info.name }}
"""

import os
import json
import torch
import numpy as np
from PIL import Image
from typing import Dict, Any

def load_model():
    """Load VQA model and processor"""
    from transformers import AutoProcessor, {{ model_type.model_class }}
    
    processor = AutoProcessor.from_pretrained("./model")
    model = {{ model_type.model_class }}.from_pretrained(
        "./model",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    return model, processor

def process_inputs(
    image_path: str,
    question: str,
    model,
    processor
) -> Dict[str, torch.Tensor]:
    """Process image and question inputs"""
    image = Image.open(image_path).convert('RGB')
    inputs = processor(
        images=image,
        text=question,
        return_tensors="pt"
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    return inputs

def format_outputs(
    outputs,
    processor,
    input_ids=None
) -> Dict[str, Any]:
    """Format model outputs"""
    if hasattr(outputs, "logits"):
        # Classification-style output
        probs = torch.nn.functional.softmax(outputs.logits[0], dim=-1)
        pred_idx = torch.argmax(probs).item()
        answer = processor.config.id2label[pred_idx]
        confidence = float(probs[pred_idx])
    else:
        # Generation-style output
        answer = processor.batch_decode(
            outputs[0],
            skip_special_tokens=True
        )[0]
        confidence = None

    return {
        "answer": answer,
        "confidence": confidence
    }

def run_vqa(
    image_path: str,
    question: str,
    model,
    processor
) -> Dict[str, Any]:
    """Run visual question answering"""
    # Process inputs
    inputs = process_inputs(image_path, question, model, processor)

    # Run inference
    with torch.no_grad():
        if hasattr(model, "generate"):
            outputs = model.generate(
                **inputs,
                max_new_tokens={{ model_config.max_new_tokens or 64 }},
                num_beams={{ model_config.num_beams or 4 }},
                temperature={{ model_config.temperature or 0.7 }}
            )
        else:
            outputs = model(**inputs)

    # Format outputs
    result = format_outputs(outputs, processor, inputs.get("input_ids"))
    
    return {
        "input_image": image_path,
        "question": question,
        "answer": result["answer"],
        "confidence": result["confidence"],
        "parameters": {
            "max_new_tokens": {{ model_config.max_new_tokens or 64 }},
            "num_beams": {{ model_config.num_beams or 4 }},
            "temperature": {{ model_config.temperature or 0.7 }}
        },
        "metadata": {
            "model_type": model.config.model_type,
            "task": model.config.task,
            "architecture": model.config.architectures[0]
        }
    }

def main():
    """Main inference function"""
    image_path = os.getenv("IMAGE_PATH", "/inputs/image.jpg")
    question = os.getenv("MODEL_INPUT", "What is shown in this image?")
    
    model, processor = load_model()
    results = run_vqa(image_path, question, model, processor)
    
    output_file = os.path.join("/outputs", "results.json")
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    main()